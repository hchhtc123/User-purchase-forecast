{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 飞桨常规赛：用户购买预测 7月第7名方案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> 说明：一开始主要使用树模型但效果上较差，后期主要是参考[飞桨常规赛：用户购买预测 3月第4名方案](https://aistudio.baidu.com/aistudio/projectdetail/1077670?channelType=0&channel=0)，在其基础上进行调参优化。\n",
    "\n",
    "> 在7月获得了该比赛的第7名，该项目主用于代码审核。需注意项目运行最好选择GPU环境，否则会出现内存不足的情况。\n",
    "鉴于排行榜中出现的较多0分情况，应该是没有看好赛题要求：输出类别标签为0或1而不是分数，需要对结果进行后处理，这点需要特别注意！\n",
    "\n",
    "> Github地址：https://github.com/hchhtc123/User-purchase-forecast\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/10e2eb8a298f42918ce1a4f051c84f04ed251b9baadd463cb444213461704c7c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**以下为大佬的基线模型，本人仅做了调参优化处理：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 赛题介绍\n",
    "智能营销工具可以帮助商家预测用户购买的行为，本次比赛提供了一份品牌商家的历史订单数据，参赛选手需构建一个预测模型，预估用户人群在规定时间内产生购买行为的概率。\n",
    "该模型可应用于各种电商数据分析，以及百度电商开放平台， 不仅可以帮助商家基于平台流量，进行商品售卖、支付，还可以通过MarTech技术更精准地锁定核心用户，对用户的购买行为进行预测。\n",
    "\n",
    "[点击跳转至赛题页面](https://aistudio.baidu.com/aistudio/competition/detail/51)\n",
    "\n",
    "## 基线介绍\n",
    "### 运行方式\n",
    "\n",
    "本次基线基于飞桨PaddlePaddle 2.1版本，若本地运行则可能需要额外安装jupyter notebook环境、pandas模块等。\n",
    "AI Studio上运行建议使用32G内存的高级版，本地运行同样建议配置较大的内存空间。\n",
    "\n",
    "#### AI Studio (Notebook)运行\n",
    "\n",
    "依次运行下方的cell即可，若运行时修改了cell，推荐在右上角重启执行器后再以此运行，避免因内存未清空而产生报错。\n",
    "\n",
    "#### 本地运行\n",
    "\n",
    "fork本项目后点击右上角的“文件”——“导出Notebook为ipynb”，下载到本地后在`jupyter notebook`环境即可开始训练，生成的推理结果文件为`result.csv`。\n",
    "\n",
    "### 设计思想\n",
    "\n",
    "#### 执行流程\n",
    "\n",
    "1. 配置预处理数据方案(选手可以自行设计，默认提供用于时间滑窗特征工程和归一化两种方案)\n",
    "2. 检查数据是否可以正确读取（可省略，若选手自行修改了数据预处理部分，务必检查能否读取后再进行下一步操作）\n",
    "3. 开始训练\n",
    "4. 执行预测并产生结果文件\n",
    "\n",
    "#### 技术方案\n",
    "在本次赛题中，虽然赛题是一个二分类任务（用户购买、未购买），但从赛题数据看，属于比较典型的时间序列数据，也可以参照以往的线性回归任务的做法处理。 \n",
    "接下来将介绍技术方案中的一些细节问题以及method流程。\n",
    "\n",
    "##### label设计\n",
    "本次赛题反映了一个客观事实——在真实场景应用机器学习/深度学习技术时，通常是没有已经整理好的训练集、验证集、测试集，需要自己设计。\n",
    "\n",
    "比如赛题中提到，在比赛任务是预测下个月用户是否购买，下个月是哪个月？我们不妨设想自己是个业务经理，现在领导说做个模型，预测下个月你手上的客户是否会流失。所以在这类题目中，下个月就是提供的数据集截止日期之后的一个月。当然，如果比赛要求预测未来7天、未来15天的销售情况，道理也是一样的。\n",
    "\n",
    "在此类比赛的解决方案中，通常会有个时间滑窗的概念。比如按月进行时间滑窗，本题中数据到2013.8.31，默认提供的数据集划分设计如下（选手也可以自行设计数据集的划分）：\n",
    "- 训练集：用2013年4-6月的数据预测用户在7月是否购买\n",
    "- 验证集：用2013年5-7月的数据预测用户在8月是否购买\n",
    "- 测试集：用2013年6-8月的数据预测用户在9月是否购买\n",
    "\n",
    "```python\n",
    "# 这是一个时间滑窗函数，获得dt之前minus天以来periods的dataframe，以便进一步计算\n",
    "def get_timespan(df, dt, minus, periods, freq='D'):\n",
    "    return df[pd.date_range(dt - timedelta(days=minus), periods=periods, freq=freq)]\n",
    "```\n",
    "\n",
    "##### 时间滑窗特征构建\n",
    "> 注：更详细的时间滑窗特征工程的方法请参考[用户购买预测时间滑窗特征构建](https://aistudio.baidu.com/aistudio/projectdetail/276829)，本项目做了大幅缩减。\n",
    "\n",
    "时间滑窗在业务应用上被称为RFM模型，RFM模型最早是用来衡量客户价值和客户创利能力。理解RFM框架的思想是构造统计类特征的基础，其含义为：\n",
    "- R（Recency）：客户最近一次交易消费时间的间隔。R值越大，表示客户交易发生的日期越久，反之则表示客户交易发生的日期越近。\n",
    "- F（Frequency）：客户在最近一段时间内交易消费的次数。F值越大，表示客户交易越频繁，反之则表示客户交易不够活跃。\n",
    "- M（Monetary）：客户在最近一段时间内交易消费的金额。M值越大，表示客户价值越高，反之则表示客户价值越低。\n",
    "\n",
    "也就是说，时间滑窗特征本身是与业务紧密联系的，而在这类时间序列数据的比赛中，滑动时间窗口内的统计指标可以更加丰富，统计值一般会有最大值、最小值、均值、标准差、中位数、极差等。\n",
    "\n",
    "```python\n",
    "# 要计算统计指标特征的时间窗口\n",
    "for i in [14,30,60,91]:\n",
    "\ttmp = get_timespan(df_payment, t2018, i, i)\n",
    "   # 削去峰值的均值特征\n",
    "   X['mean_%s_decay' % i] = (tmp * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\n",
    "   # 中位数特征，在本赛题中基本不适用\n",
    "   # X['median_%s' % i] = tmp.median(axis=1).values\n",
    "   # 最小值特征，在本赛题中基本不适用\n",
    "   # X['min_%s' % i] = tmp_1.min(axis=1).values\n",
    "   # 最大值特征\n",
    "   X['max_%s' % i] = tmp.max(axis=1).values\n",
    "   # 标准差特征\n",
    "   # X['std_%s' % i] = tmp_1.std(axis=1).values\n",
    "   # 求和特征\n",
    "   X['sum_%s' % i] = tmp.sum(axis=1).values\n",
    "```\n",
    "##### 深度学习模型搭建\n",
    "参考[使用飞桨重写房价预测模型](https://aistudio.baidu.com/aistudio/projectdetail/366426)和[Martech_track1_beta](https://aistudio.baidu.com/aistudio/projectdetail/510779)搭建三层深度神经网络。需要注意的是，由于神经网络对缺失值和稀疏数据敏感，对送入神经网络的特征需要做筛选。另外，选择哪种神经网络结构效果更好，需要参赛选手进一步探索。\n",
    "```python\n",
    "# 构建多层神经网络\n",
    "class Regressor(fluid.dygraph.Layer):\n",
    "    def __init__(self, name_scope):\n",
    "        super(Regressor, self).__init__(name_scope)\n",
    "        name_scope = self.full_name()\n",
    "        # 定义三层全连接层，输入维度是最终选取的特征数量，输出维度是1，激活函数为relu\n",
    "        self.fc1 = Linear(input_dim=41, output_dim=128, act='relu') # 输入层，input dim 为数据维度大小\n",
    "        self.fc2 = Linear(input_dim=128, output_dim=128, act='relu')\n",
    "        self.fc3 = Linear(input_dim=128, output_dim=1, act='sigmoid')\n",
    "    # 网络的前向计算函数\n",
    "    def forward(self, inputs):\n",
    "        fc1 = self.fc1(inputs)\n",
    "        fc2 = self.fc2(fc1)\n",
    "        x = self.fc3(fc2)\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据预处理 - 数据集划分与特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 导入所需的第三方库\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "from datetime import datetime, date, timedelta\n",
    "from scipy.stats import skew\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from itertools import product\n",
    "import datetime as dt\n",
    "import calendar\n",
    "import gc\n",
    "from datetime import date, timedelta\n",
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "import paddle.fluid.dygraph as dygraph\n",
    "from paddle.fluid.dygraph import Linear\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数据集位置\n",
    "PATH = './data/data19383/'\n",
    "train = pd.read_csv(PATH + 'train.csv')\n",
    "# train = pd.read_csv('./data/data19383/train.csv', usecols=[2, 3, 4, 6, 7, 18])\n",
    "# set index to ID to avoid droping it later\n",
    "# 把测试集的id列作为索引，防止误删\n",
    "test  = pd.read_csv(PATH + 'submission.csv').set_index('customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_detail_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_amount</th>\n",
       "      <th>order_pay_time</th>\n",
       "      <th>is_customer_rate</th>\n",
       "      <th>order_detail_goods_num</th>\n",
       "      <th>order_detail_amount</th>\n",
       "      <th>order_detail_discount</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>goods_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2306861</th>\n",
       "      <td>3685490</td>\n",
       "      <td>3238357</td>\n",
       "      <td>707.7</td>\n",
       "      <td>2013-01-24 00:24:40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>179.8</td>\n",
       "      <td>298.2</td>\n",
       "      <td>2826572</td>\n",
       "      <td>1478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306862</th>\n",
       "      <td>3685491</td>\n",
       "      <td>3238356</td>\n",
       "      <td>775.9</td>\n",
       "      <td>2012-11-11 17:35:05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126.9</td>\n",
       "      <td>172.1</td>\n",
       "      <td>2826572</td>\n",
       "      <td>2103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306863</th>\n",
       "      <td>3685492</td>\n",
       "      <td>3238357</td>\n",
       "      <td>707.7</td>\n",
       "      <td>2013-01-24 00:24:40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.9</td>\n",
       "      <td>2826572</td>\n",
       "      <td>3153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306864</th>\n",
       "      <td>3685493</td>\n",
       "      <td>3238356</td>\n",
       "      <td>775.9</td>\n",
       "      <td>2012-11-11 17:35:05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>74.9</td>\n",
       "      <td>105.0</td>\n",
       "      <td>2826572</td>\n",
       "      <td>1778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306865</th>\n",
       "      <td>3685494</td>\n",
       "      <td>3238357</td>\n",
       "      <td>707.7</td>\n",
       "      <td>2013-01-24 00:24:40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>94.9</td>\n",
       "      <td>104.9</td>\n",
       "      <td>2826572</td>\n",
       "      <td>2128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306866</th>\n",
       "      <td>3685495</td>\n",
       "      <td>3238358</td>\n",
       "      <td>199.0</td>\n",
       "      <td>2013-01-10 19:24:31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.9</td>\n",
       "      <td>139.1</td>\n",
       "      <td>2826573</td>\n",
       "      <td>1173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306867</th>\n",
       "      <td>3685496</td>\n",
       "      <td>3238359</td>\n",
       "      <td>299.8</td>\n",
       "      <td>2013-01-27 15:00:27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.9</td>\n",
       "      <td>2826574</td>\n",
       "      <td>2513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306868</th>\n",
       "      <td>3685497</td>\n",
       "      <td>3238359</td>\n",
       "      <td>299.8</td>\n",
       "      <td>2013-01-27 15:00:27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>89.9</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2826574</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306869</th>\n",
       "      <td>3685498</td>\n",
       "      <td>3238360</td>\n",
       "      <td>168.0</td>\n",
       "      <td>2012-11-11 00:10:37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>76.9</td>\n",
       "      <td>91.1</td>\n",
       "      <td>2826574</td>\n",
       "      <td>1423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306870</th>\n",
       "      <td>3685499</td>\n",
       "      <td>3238361</td>\n",
       "      <td>102.0</td>\n",
       "      <td>2013-07-10 14:22:14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.9</td>\n",
       "      <td>52.1</td>\n",
       "      <td>2826574</td>\n",
       "      <td>1043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         order_detail_id  order_id  order_amount       order_pay_time  \\\n",
       "2306861          3685490   3238357         707.7  2013-01-24 00:24:40   \n",
       "2306862          3685491   3238356         775.9  2012-11-11 17:35:05   \n",
       "2306863          3685492   3238357         707.7  2013-01-24 00:24:40   \n",
       "2306864          3685493   3238356         775.9  2012-11-11 17:35:05   \n",
       "2306865          3685494   3238357         707.7  2013-01-24 00:24:40   \n",
       "2306866          3685495   3238358         199.0  2013-01-10 19:24:31   \n",
       "2306867          3685496   3238359         299.8  2013-01-27 15:00:27   \n",
       "2306868          3685497   3238359         299.8  2013-01-27 15:00:27   \n",
       "2306869          3685498   3238360         168.0  2012-11-11 00:10:37   \n",
       "2306870          3685499   3238361         102.0  2013-07-10 14:22:14   \n",
       "\n",
       "         is_customer_rate  order_detail_goods_num  order_detail_amount  \\\n",
       "2306861               0.0                     2.0                179.8   \n",
       "2306862               0.0                     1.0                126.9   \n",
       "2306863               0.0                     1.0                  0.0   \n",
       "2306864               0.0                     1.0                 74.9   \n",
       "2306865               0.0                     1.0                 94.9   \n",
       "2306866               0.0                     1.0                 59.9   \n",
       "2306867               0.0                     1.0                  0.0   \n",
       "2306868               0.0                     1.0                 89.9   \n",
       "2306869               0.0                     1.0                 76.9   \n",
       "2306870               0.0                     1.0                 49.9   \n",
       "\n",
       "         order_detail_discount  customer_id  goods_id  \n",
       "2306861                  298.2      2826572      1478  \n",
       "2306862                  172.1      2826572      2103  \n",
       "2306863                   29.9      2826572      3153  \n",
       "2306864                  105.0      2826572      1778  \n",
       "2306865                  104.9      2826572      2128  \n",
       "2306866                  139.1      2826573      1173  \n",
       "2306867                   59.9      2826574      2513  \n",
       "2306868                  150.0      2826574       998  \n",
       "2306869                   91.1      2826574      1423  \n",
       "2306870                   52.1      2826574      1043  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练集概况，查看最后面10条\r\n",
    "train[['order_detail_id','order_id','order_amount','order_pay_time','is_customer_rate','order_detail_goods_num','order_detail_amount','order_detail_discount','customer_id','goods_id']].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/cbook/__init__.py:2349: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  if isinstance(obj, collections.Iterator):\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/cbook/__init__.py:2366: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return list(data) if isinstance(data, collections.MappingView) else data\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 查看类别标签分布情况\r\n",
    "%matplotlib inline\r\n",
    "y = range(1,2)\r\n",
    " \r\n",
    "plt.bar(['not buy','buy'], [1585986-174770,174770], alpha=0.5, width=0.3, color='lightblue', edgecolor='grey', lw=3)\r\n",
    "plt.title('Sales in August 2013', fontsize=10)\r\n",
    "for a, b in zip(['not buy','buy'], [1585986-174770,174770]):\r\n",
    "    plt.text(a, b + 0.05, '%.0f' % b, ha='center', va='bottom', fontsize=10)\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集使用内容 510.40096282958984 MB\n",
      "测试集使用内存 24.200225830078125 MB\n"
     ]
    }
   ],
   "source": [
    "# 对于特别大的文件，我们需要做一些内存检查\n",
    "mem_train = train.memory_usage(index=True).sum()\n",
    "mem_test=test.memory_usage(index=True).sum()\n",
    "print(u\"训练集使用内容 \"+ str(mem_train/ 1024**2)+\" MB\")\n",
    "print(u\"测试集使用内存 \"+ str(mem_test/ 1024**2)+\" MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 内存优化脚本\n",
    "- 参考[缓解pandas中DataFrame占用内存过大的问题](https://blog.csdn.net/wj1066/article/details/81124959)\n",
    "- 效果非常显著，有效避免内存溢出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# @from: https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65/code\r\n",
    "# @liscense: Apache 2.0\r\n",
    "# @author: weijian\r\n",
    "def reduce_mem_usage(props):\r\n",
    "    # 计算当前内存\r\n",
    "    start_mem_usg = props.memory_usage().sum() / 1024 ** 2\r\n",
    "    print(\"Memory usage of the dataframe is :\", start_mem_usg, \"MB\")\r\n",
    "    \r\n",
    "    # 哪些列包含空值，空值用-999填充。why：因为np.nan当做float处理\r\n",
    "    NAlist = []\r\n",
    "    for col in props.columns:\r\n",
    "        # 这里只过滤了objectd格式，如果你的代码中还包含其他类型，请一并过滤\r\n",
    "        if (props[col].dtypes != object):\r\n",
    "            \r\n",
    "            # print(\"**************************\")\r\n",
    "            # print(\"columns: \", col)\r\n",
    "            # print(\"dtype before\", props[col].dtype)\r\n",
    "            \r\n",
    "            # 判断是否是int类型\r\n",
    "            isInt = False\r\n",
    "            mmax = props[col].max()\r\n",
    "            mmin = props[col].min()\r\n",
    "            \r\n",
    "            # Integer does not support NA, therefore Na needs to be filled\r\n",
    "            if not np.isfinite(props[col]).all():\r\n",
    "                NAlist.append(col)\r\n",
    "                props[col].fillna(-999, inplace=True) # 用-999填充\r\n",
    "                \r\n",
    "            # test if column can be converted to an integer\r\n",
    "            asint = props[col].fillna(0).astype(np.int64)\r\n",
    "            result = np.fabs(props[col] - asint)\r\n",
    "            result = result.sum()\r\n",
    "            if result < 0.01: # 绝对误差和小于0.01认为可以转换的，要根据task修改\r\n",
    "                isInt = True\r\n",
    "            \r\n",
    "            # make interger / unsigned Integer datatypes\r\n",
    "            if isInt:\r\n",
    "                if mmin >= 0: # 最小值大于0，转换成无符号整型\r\n",
    "                    if mmax <= 255:\r\n",
    "                        props[col] = props[col].astype(np.uint8)\r\n",
    "                    elif mmax <= 65535:\r\n",
    "                        props[col] = props[col].astype(np.uint16)\r\n",
    "                    elif mmax <= 4294967295:\r\n",
    "                        props[col] = props[col].astype(np.uint32)\r\n",
    "                    else:\r\n",
    "                        props[col] = props[col].astype(np.uint64)\r\n",
    "                else: # 转换成有符号整型\r\n",
    "                    if mmin > np.iinfo(np.int8).min and mmax < np.iinfo(np.int8).max:\r\n",
    "                        props[col] = props[col].astype(np.int8)\r\n",
    "                    elif mmin > np.iinfo(np.int16).min and mmax < np.iinfo(np.int16).max:\r\n",
    "                        props[col] = props[col].astype(np.int16)\r\n",
    "                    elif mmin > np.iinfo(np.int32).min and mmax < np.iinfo(np.int32).max:\r\n",
    "                        props[col] = props[col].astype(np.int32)\r\n",
    "                    elif mmin > np.iinfo(np.int64).min and mmax < np.iinfo(np.int64).max:\r\n",
    "                        props[col] = props[col].astype(np.int64)  \r\n",
    "            else: # 注意：这里对于float都转换成float16，需要根据你的情况自己更改\r\n",
    "                props[col] = props[col].astype(np.float16)\r\n",
    "            \r\n",
    "            # print(\"dtype after\", props[col].dtype)\r\n",
    "            # print(\"********************************\")\r\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\r\n",
    "    mem_usg = props.memory_usage().sum() / 1024**2 \r\n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\r\n",
    "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\r\n",
    "    return props, NAlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集使用内容 349.8006982803345 MB\n",
      "测试集使用内存 24.200225830078125 MB\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理\n",
    "# 处理id字段\n",
    "train['order_detail_id'] = train['order_detail_id'].astype(np.uint32)\n",
    "train['order_id'] = train['order_id'].astype(np.uint32)\n",
    "train['customer_id'] = train['customer_id'].astype(np.uint32)\n",
    "train['goods_id'] = train['goods_id'].astype(np.uint32)\n",
    "train['goods_class_id'] = train['goods_class_id'].astype(np.uint32)\n",
    "train['member_id'] = train['member_id'].astype(np.uint32)\n",
    "# 处理状态字段，这里同时处理空值，将空值置为0\n",
    "train['order_status'] = train['order_status'].astype(np.uint8)\n",
    "train['goods_has_discount'] = train['goods_has_discount'].astype(np.uint8)\n",
    "train[\"is_member_actived\"].fillna(0, inplace=True)\n",
    "train[\"is_member_actived\"]=train[\"is_member_actived\"].astype(np.int8)\n",
    "train[\"member_status\"].fillna(0, inplace=True)\n",
    "train[\"member_status\"]=train[\"member_status\"].astype(np.int8)\n",
    "train[\"customer_gender\"].fillna(0, inplace=True)\n",
    "train[\"customer_gender\"]=train[\"customer_gender\"].astype(np.int8)\n",
    "train['is_customer_rate'] = train['is_customer_rate'].astype(np.uint8)\n",
    "train['order_detail_status'] = train['order_detail_status'].astype(np.uint8)\n",
    "# 处理日期\n",
    "train['goods_list_time']=pd.to_datetime(train['goods_list_time'],format=\"%Y-%m-%d\")\n",
    "train['order_pay_time']=pd.to_datetime(train['order_pay_time'],format=\"%Y-%m-%d\")\n",
    "train['goods_delist_time']=pd.to_datetime(train['goods_delist_time'],format=\"%Y-%m-%d\")\n",
    "# 检查内存使用\n",
    "mem_train = train.memory_usage(index=True).sum()\n",
    "mem_test=test.memory_usage(index=True).sum()\n",
    "print(u\"训练集使用内容 \"+ str(mem_train/ 1024**2)+\" MB\")\n",
    "print(u\"测试集使用内存 \"+ str(mem_test/ 1024**2)+\" MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 构造时间滑窗特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 每日付款金额"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# 将用户下单金额按天进行汇总\r\n",
    "# df = train[train.order_status<101][train.order_pay_time>'2013-02-01']\r\n",
    "df = train[train.order_pay_time>'2013-02-01']\r\n",
    "df['date'] = pd.DatetimeIndex(df['order_pay_time']).date\r\n",
    "df_payment = df[['customer_id','date','order_total_payment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "注意，成功交易的客户数量不等于全部客户数量，说明有相当一部分客户虽然下过单，但是没有成功的订单，那么这些客户自然应当算在训练集之外。\n",
    "数据合并时，由于`test.csv`中，已经设置了默认0值，只需要和训练后的预测标签做一个`left join`就可以了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_payment = df_payment.groupby(['date','customer_id']).agg({'order_total_payment': ['sum']})\n",
    "df_payment.columns = ['day_total_payment']\n",
    "df_payment.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_payment = df_payment.set_index(\n",
    "    [\"customer_id\", \"date\"])[[\"day_total_payment\"]].unstack(level=-1).fillna(0)\n",
    "df_payment.columns = df_payment.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 每日购买商品数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_goods = df[['customer_id','date','order_total_num']]\r\n",
    "df_goods = df_goods.groupby(['date','customer_id']).agg({'order_total_num': ['sum']})\r\n",
    "df_goods.columns = ['day_total_num']\r\n",
    "df_goods.reset_index(inplace=True)\r\n",
    "df_goods = df_goods.set_index(\r\n",
    "    [\"customer_id\", \"date\"])[[\"day_total_num\"]].unstack(level=-1).fillna(0)\r\n",
    "df_goods.columns = df_goods.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "该场景每天都有成交记录，这样就不需要考虑生成完整时间段填充的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 这是一个时间滑窗函数，获得dt之前minus天以来periods的dataframe，以便进一步计算\n",
    "def get_timespan(df, dt, minus, periods, freq='D'):\n",
    "    return df[pd.date_range(dt - timedelta(days=minus), periods=periods, freq=freq)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. 构造dataset这里有个取巧的地方，因为要预测的9月份除了开学季以外不是非常特殊的月份，因此主要考虑近期的因素，数据集的开始时间也是2月1日，尽量避免了双十一、元旦假期的影响，当然春节假期继续保留。同时，构造数据集的时候保留了customer_id，主要为了与其它特征做整合。\n",
    "2. 通过一个函数整合付款金额和商品数量的时间滑窗，主要是因为分开做到时候合并占用内存更大，并且函数最后在返回值处做了内存优化，用时间代价尽可能避免内存溢出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(df_payment, df_goods, t2018, is_train=True):\r\n",
    "    X = {}\r\n",
    "    # 整合用户id\r\n",
    "    tmp = df_payment.reset_index()\r\n",
    "    X['customer_id'] = tmp['customer_id']\r\n",
    "    # 消费特征\r\n",
    "    print('Preparing payment feature...')\r\n",
    "    for i in [14,30,60,91]:\r\n",
    "        tmp = get_timespan(df_payment, t2018, i, i)\r\n",
    "        # X['diff_%s_mean' % i] = tmp_1.diff(axis=1).mean(axis=1).values\r\n",
    "        X['mean_%s_decay' % i] = (tmp * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\r\n",
    "        # X['mean_%s' % i] = tmp_1.mean(axis=1).values\r\n",
    "        # X['median_%s' % i] = tmp.median(axis=1).values\r\n",
    "        # X['min_%s' % i] = tmp_1.min(axis=1).values\r\n",
    "        X['max_%s' % i] = tmp.max(axis=1).values\r\n",
    "        # X['std_%s' % i] = tmp_1.std(axis=1).values\r\n",
    "        X['sum_%s' % i] = tmp.sum(axis=1).values\r\n",
    "    for i in [14,30,60,91]:\r\n",
    "        tmp = get_timespan(df_payment, t2018 + timedelta(days=-7), i, i)\r\n",
    "        X['mean_%s_decay_2' % i] = (tmp * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\r\n",
    "        # X['mean_%s_2' % i] = tmp_2.mean(axis=1).values\r\n",
    "        # X['median_%s_2' % i] = tmp.median(axis=1).values\r\n",
    "        # X['min_%s_2' % i] = tmp_2.min(axis=1).values\r\n",
    "        X['max_%s_2' % i] = tmp.max(axis=1).values\r\n",
    "        # X['std_%s_2' % i] = tmp_2.std(axis=1).values\r\n",
    "    for i in [14,30,60,91]:\r\n",
    "        tmp = get_timespan(df_payment, t2018, i, i)\r\n",
    "        X['has_sales_days_in_last_%s' % i] = (tmp != 0).sum(axis=1).values\r\n",
    "        X['last_has_sales_day_in_last_%s' % i] = i - ((tmp != 0) * np.arange(i)).max(axis=1).values\r\n",
    "        X['first_has_sales_day_in_last_%s' % i] = ((tmp != 0) * np.arange(i, 0, -1)).max(axis=1).values\r\n",
    "\r\n",
    "    # 对此处进行微调，主要考虑近期因素\r\n",
    "    for i in range(1, 4):\r\n",
    "        X['day_%s_2018' % i] = get_timespan(df_payment, t2018, i*30, 30).sum(axis=1).values\r\n",
    "    # 商品数量特征，这里故意把时间和消费特征错开，提高时间滑窗的覆盖面\r\n",
    "    print('Preparing num feature...')\r\n",
    "    for i in [21,49,84]:\r\n",
    "            tmp = get_timespan(df_goods, t2018, i, i)\r\n",
    "            # X['goods_diff_%s_mean' % i] = tmp_1.diff(axis=1).mean(axis=1).values\r\n",
    "            # X['goods_mean_%s_decay' % i] = (tmp_1 * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\r\n",
    "            X['goods_mean_%s' % i] = tmp.mean(axis=1).values\r\n",
    "            # X['goods_median_%s' % i] = tmp.median(axis=1).values\r\n",
    "            # X['goods_min_%s' % i] = tmp_1.min(axis=1).values\r\n",
    "            X['goods_max_%s' % i] = tmp.max(axis=1).values\r\n",
    "            # X['goods_std_%s' % i] = tmp_1.std(axis=1).values\r\n",
    "            X['goods_sum_%s' % i] = tmp.sum(axis=1).values\r\n",
    "    for i in [21,49,84]:    \r\n",
    "            tmp = get_timespan(df_goods, t2018 + timedelta(weeks=-1), i, i)\r\n",
    "            # X['goods_diff_%s_mean_2' % i] = tmp_2.diff(axis=1).mean(axis=1).values\r\n",
    "            # X['goods_mean_%s_decay_2' % i] = (tmp_2 * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\r\n",
    "            X['goods_mean_%s_2' % i] = tmp.mean(axis=1).values\r\n",
    "            # X['goods_median_%s_2' % i] = tmp.median(axis=1).values\r\n",
    "            # X['goods_min_%s_2' % i] = tmp_2.min(axis=1).values\r\n",
    "            X['goods_max_%s_2' % i] = tmp.max(axis=1).values\r\n",
    "            X['goods_sum_%s_2' % i] = tmp.sum(axis=1).values\r\n",
    "    for i in [21,49,84]:    \r\n",
    "            tmp = get_timespan(df_goods, t2018, i, i)\r\n",
    "            X['goods_has_sales_days_in_last_%s' % i] = (tmp > 0).sum(axis=1).values\r\n",
    "            X['goods_last_has_sales_day_in_last_%s' % i] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values\r\n",
    "            X['goods_first_has_sales_day_in_last_%s' % i] = ((tmp > 0) * np.arange(i, 0, -1)).max(axis=1).values\r\n",
    "\r\n",
    "\r\n",
    "    # 对此处进行微调，主要考虑近期因素\r\n",
    "    for i in range(1, 4):\r\n",
    "        X['goods_day_%s_2018' % i] = get_timespan(df_goods, t2018, i*28, 28).sum(axis=1).values\r\n",
    "\r\n",
    "    X = pd.DataFrame(X)\r\n",
    "    \r\n",
    "    reduce_mem_usage(X)\r\n",
    "    \r\n",
    "    if is_train:\r\n",
    "        # 这样转换之后，打标签直接用numpy切片就可以了\r\n",
    "        # 当然这里前提是确认付款总额没有负数的问题\r\n",
    "        X['label'] = df_goods[pd.date_range(t2018, periods=30)].max(axis=1).values\r\n",
    "        X['label'][X['label'] > 0] = 1\r\n",
    "        return X\r\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing payment feature...\n",
      "Preparing num feature...\n",
      "Memory usage of the dataframe is : 345.16221618652344 MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  73.87003993988037  MB\n",
      "This is  21.401542948710667 % of the initial size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing payment feature...\n",
      "Preparing num feature...\n",
      "Memory usage of the dataframe is : 345.16221618652344 MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  73.87003993988037  MB\n",
      "This is  21.401542948710667 % of the initial size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing payment feature...\n",
      "Preparing num feature...\n",
      "Memory usage of the dataframe is : 345.16221618652344 MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  73.87003993988037  MB\n",
      "This is  21.401542948710667 % of the initial size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing payment feature...\n",
      "Preparing num feature...\n",
      "Memory usage of the dataframe is : 345.16221618652344 MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  73.87003993988037  MB\n",
      "This is  21.401542948710667 % of the initial size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# 需要一定的时间，请耐心等待\r\n",
    "num_days = 4\r\n",
    "t2017 = date(2013, 7, 1)\r\n",
    "X_l, y_l = [], []\r\n",
    "for i in range(num_days):\r\n",
    "    delta = timedelta(days=7 * i)\r\n",
    "    # X_tmp, y_tmp = prepare_dataset(df_payment, df_goods, t2017 + delta)\r\n",
    "    X_tmp = prepare_dataset(df_payment, df_goods, t2017 + delta)\r\n",
    "    X_tmp = pd.concat([X_tmp], axis=1)\r\n",
    "\r\n",
    "    X_l.append(X_tmp)\r\n",
    "    # y_l.append(y_tmp)\r\n",
    "\r\n",
    "X_train = pd.concat(X_l, axis=0)\r\n",
    "# y_train = np.concatenate(y_l, axis=0)\r\n",
    "\r\n",
    "del X_l, y_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing payment feature...\n",
      "Preparing num feature...\n",
      "Memory usage of the dataframe is : 345.16221618652344 MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  73.87003993988037  MB\n",
      "This is  21.401542948710667 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "X_test = prepare_dataset(df_payment, df_goods, date(2013, 9, 1), is_train=False)\r\n",
    "X_test = pd.concat([X_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 中间结果保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test.to_csv('X_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练&推理 - 训练配置以及训练\n",
    "### 加载特征工程结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_train = pd.read_csv('X_train.csv',usecols=['max_30','has_sales_days_in_last_30','first_has_sales_day_in_last_60','goods_sum_49','label'])\r\n",
    "X_train = pd.read_csv('X_train.csv')\r\n",
    "X_train.drop(['Unnamed: 0','customer_id'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_val = pd.read_csv('X_test.csv')\r\n",
    "X_val.drop('Unnamed: 0', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('X_test.csv')\r\n",
    "# X_test = pd.read_csv('X_test.csv',usecols=['max_30','has_sales_days_in_last_30','first_has_sales_day_in_last_60','goods_sum_49'])\r\n",
    "X_test.drop(['Unnamed: 0','customer_id'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_14_decay</th>\n",
       "      <th>max_14</th>\n",
       "      <th>sum_14</th>\n",
       "      <th>mean_30_decay</th>\n",
       "      <th>max_30</th>\n",
       "      <th>sum_30</th>\n",
       "      <th>mean_60_decay</th>\n",
       "      <th>max_60</th>\n",
       "      <th>sum_60</th>\n",
       "      <th>mean_91_decay</th>\n",
       "      <th>...</th>\n",
       "      <th>goods_has_sales_days_in_last_49</th>\n",
       "      <th>goods_last_has_sales_day_in_last_49</th>\n",
       "      <th>goods_first_has_sales_day_in_last_49</th>\n",
       "      <th>goods_has_sales_days_in_last_84</th>\n",
       "      <th>goods_last_has_sales_day_in_last_84</th>\n",
       "      <th>goods_first_has_sales_day_in_last_84</th>\n",
       "      <th>goods_day_1_2018</th>\n",
       "      <th>goods_day_2_2018</th>\n",
       "      <th>goods_day_3_2018</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.537</td>\n",
       "      <td>39.9</td>\n",
       "      <td>39.9</td>\n",
       "      <td>3.537</td>\n",
       "      <td>39.9</td>\n",
       "      <td>39.9</td>\n",
       "      <td>3.537</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.530</td>\n",
       "      <td>98.9</td>\n",
       "      <td>197.8</td>\n",
       "      <td>2.530</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_14_decay  max_14  sum_14  mean_30_decay  max_30  sum_30  \\\n",
       "0            0.0     0.0     0.0          0.000     0.0     0.0   \n",
       "1            0.0     0.0     0.0          0.000     0.0     0.0   \n",
       "2            0.0     0.0     0.0          3.537    39.9    39.9   \n",
       "3            0.0     0.0     0.0          0.000     0.0     0.0   \n",
       "4            0.0     0.0     0.0          0.000     0.0     0.0   \n",
       "\n",
       "   mean_60_decay  max_60  sum_60  mean_91_decay  ...  \\\n",
       "0          0.000     0.0     0.0          0.000  ...   \n",
       "1          0.000     0.0     0.0          0.000  ...   \n",
       "2          3.537    39.9    39.9          3.537  ...   \n",
       "3          0.000     0.0     0.0          0.000  ...   \n",
       "4          2.530    98.9   197.8          2.530  ...   \n",
       "\n",
       "   goods_has_sales_days_in_last_49  goods_last_has_sales_day_in_last_49  \\\n",
       "0                                0                                   49   \n",
       "1                                0                                   49   \n",
       "2                                1                                   24   \n",
       "3                                0                                   49   \n",
       "4                                1                                   37   \n",
       "\n",
       "   goods_first_has_sales_day_in_last_49  goods_has_sales_days_in_last_84  \\\n",
       "0                                     0                                0   \n",
       "1                                     0                                0   \n",
       "2                                    24                                1   \n",
       "3                                     0                                0   \n",
       "4                                    37                                2   \n",
       "\n",
       "   goods_last_has_sales_day_in_last_84  goods_first_has_sales_day_in_last_84  \\\n",
       "0                                   84                                     0   \n",
       "1                                   84                                     0   \n",
       "2                                   24                                    24   \n",
       "3                                   84                                     0   \n",
       "4                                   37                                    56   \n",
       "\n",
       "   goods_day_1_2018  goods_day_2_2018  goods_day_3_2018  label  \n",
       "0                 0                 0                 0    0.0  \n",
       "1                 0                 0                 0    0.0  \n",
       "2                 1                 0                 0    0.0  \n",
       "3                 0                 0                 0    1.0  \n",
       "4                 0                 2                 0    1.0  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 选取参与训练的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mean_14_decay', 'max_14', 'sum_14', 'mean_30_decay', 'max_30',\n",
      "       'sum_30', 'mean_60_decay', 'max_60', 'sum_60', 'mean_91_decay',\n",
      "       'max_91', 'sum_91', 'mean_14_decay_2', 'max_14_2', 'mean_30_decay_2',\n",
      "       'max_30_2', 'mean_60_decay_2', 'max_60_2', 'mean_91_decay_2',\n",
      "       'max_91_2', 'has_sales_days_in_last_14',\n",
      "       'last_has_sales_day_in_last_14', 'first_has_sales_day_in_last_14',\n",
      "       'has_sales_days_in_last_30', 'last_has_sales_day_in_last_30',\n",
      "       'first_has_sales_day_in_last_30', 'has_sales_days_in_last_60',\n",
      "       'last_has_sales_day_in_last_60', 'first_has_sales_day_in_last_60',\n",
      "       'has_sales_days_in_last_91', 'last_has_sales_day_in_last_91',\n",
      "       'first_has_sales_day_in_last_91', 'day_1_2018', 'day_2_2018',\n",
      "       'day_3_2018', 'goods_mean_21', 'goods_max_21', 'goods_sum_21',\n",
      "       'goods_mean_49', 'goods_max_49', 'goods_sum_49', 'goods_mean_84',\n",
      "       'goods_max_84', 'goods_sum_84', 'goods_mean_21_2', 'goods_max_21_2',\n",
      "       'goods_sum_21_2', 'goods_mean_49_2', 'goods_max_49_2', 'goods_sum_49_2',\n",
      "       'goods_mean_84_2', 'goods_max_84_2', 'goods_sum_84_2',\n",
      "       'goods_has_sales_days_in_last_21',\n",
      "       'goods_last_has_sales_day_in_last_21',\n",
      "       'goods_first_has_sales_day_in_last_21',\n",
      "       'goods_has_sales_days_in_last_49',\n",
      "       'goods_last_has_sales_day_in_last_49',\n",
      "       'goods_first_has_sales_day_in_last_49',\n",
      "       'goods_has_sales_days_in_last_84',\n",
      "       'goods_last_has_sales_day_in_last_84',\n",
      "       'goods_first_has_sales_day_in_last_84', 'goods_day_1_2018',\n",
      "       'goods_day_2_2018', 'goods_day_3_2018', 'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train[['has_sales_days_in_last_14',\n",
    "       'last_has_sales_day_in_last_14', 'first_has_sales_day_in_last_14',\n",
    "       'has_sales_days_in_last_30', 'last_has_sales_day_in_last_30',\n",
    "       'first_has_sales_day_in_last_30', 'has_sales_days_in_last_60',\n",
    "       'last_has_sales_day_in_last_60', 'first_has_sales_day_in_last_60',\n",
    "       'has_sales_days_in_last_91', 'last_has_sales_day_in_last_91','goods_mean_21', 'goods_max_21', 'goods_sum_21',\n",
    "       'goods_mean_49', 'goods_max_49', 'goods_sum_49', 'goods_mean_84',\n",
    "       'goods_max_84', 'goods_sum_84', 'goods_mean_21_2', 'goods_max_21_2',\n",
    "       'goods_sum_21_2', 'goods_mean_49_2', 'goods_max_49_2', 'goods_sum_49_2',\n",
    "       'goods_mean_84_2', 'goods_max_84_2', 'goods_sum_84_2',\n",
    "       'goods_has_sales_days_in_last_21',\n",
    "       'goods_last_has_sales_day_in_last_21',\n",
    "       'goods_first_has_sales_day_in_last_21',\n",
    "       'goods_has_sales_days_in_last_49',\n",
    "       'goods_last_has_sales_day_in_last_49',\n",
    "       'goods_first_has_sales_day_in_last_49',\n",
    "       'goods_has_sales_days_in_last_84',\n",
    "       'goods_last_has_sales_day_in_last_84',\n",
    "       'goods_first_has_sales_day_in_last_84', 'goods_day_1_2018',\n",
    "       'goods_day_2_2018', 'goods_day_3_2018','label']]\n",
    "X_test = X_test[['has_sales_days_in_last_14',\n",
    "       'last_has_sales_day_in_last_14', 'first_has_sales_day_in_last_14',\n",
    "       'has_sales_days_in_last_30', 'last_has_sales_day_in_last_30',\n",
    "       'first_has_sales_day_in_last_30', 'has_sales_days_in_last_60',\n",
    "       'last_has_sales_day_in_last_60', 'first_has_sales_day_in_last_60',\n",
    "       'has_sales_days_in_last_91', 'last_has_sales_day_in_last_91','goods_mean_21', 'goods_max_21', 'goods_sum_21',\n",
    "       'goods_mean_49', 'goods_max_49', 'goods_sum_49', 'goods_mean_84',\n",
    "       'goods_max_84', 'goods_sum_84', 'goods_mean_21_2', 'goods_max_21_2',\n",
    "       'goods_sum_21_2', 'goods_mean_49_2', 'goods_max_49_2', 'goods_sum_49_2',\n",
    "       'goods_mean_84_2', 'goods_max_84_2', 'goods_sum_84_2',\n",
    "       'goods_has_sales_days_in_last_21',\n",
    "       'goods_last_has_sales_day_in_last_21',\n",
    "       'goods_first_has_sales_day_in_last_21',\n",
    "       'goods_has_sales_days_in_last_49',\n",
    "       'goods_last_has_sales_day_in_last_49',\n",
    "       'goods_first_has_sales_day_in_last_49',\n",
    "       'goods_has_sales_days_in_last_84',\n",
    "       'goods_last_has_sales_day_in_last_84',\n",
    "       'goods_first_has_sales_day_in_last_84', 'goods_day_1_2018',\n",
    "       'goods_day_2_2018', 'goods_day_3_2018']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index(drop=True)\r\n",
    "X_test = X_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = (X_train - X_train.min()) / (X_train.max() - X_train.min())\r\n",
    "X_test = (X_test - X_test.min()) / (X_test.max() - X_test.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 前面标签也被归一化了，还原\r\n",
    "X_train['label'][X_train['label'] > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数据分割 9:1\n",
    "def load_data(df,istrain):\n",
    "    # data = np.fromfile(datafile)\n",
    "    data = df\n",
    "\n",
    "    feature_num = len(data.columns)\n",
    "    # 将原始数据进行Reshape\n",
    "    data = np.array(data)\n",
    "    data = data.reshape([-1, feature_num])\n",
    "    \n",
    "    # 训练集和测试集的划分比例\n",
    "    #ratio = 0.9\n",
    "    if istrain == True:\n",
    "        ratio = 0.9\n",
    "        offset = int(data.shape[0] * ratio)\n",
    "        training_data = data[:offset]\n",
    "        test_data = data[offset:]\n",
    "    else:\n",
    "        training_data = data\n",
    "        test_data = None\n",
    "\n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set done.\n",
      "test set done.\n"
     ]
    }
   ],
   "source": [
    "# 加载处理后的数据\n",
    "training_data, test_data = load_data(X_train,True)\n",
    "print('train set done.')\n",
    "\n",
    "pre_data, none = load_data(X_test,False)\n",
    "print('test set done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 搭建多层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 构建3层神经网络\r\n",
    "class Regressor(fluid.dygraph.Layer):\r\n",
    "    def __init__(self, name_scope):\r\n",
    "        super(Regressor, self).__init__(name_scope)\r\n",
    "        name_scope = self.full_name()\r\n",
    "        # 定义三层全连接层，输出维度是1，激活函数为relu\r\n",
    "        self.fc1 = Linear(input_dim=41, output_dim=128, act='relu') # 输入层，input dim 为数据维度大小\r\n",
    "        self.fc2 = Linear(input_dim=128, output_dim=128, act='relu')\r\n",
    "        self.fc3 = Linear(input_dim=128, output_dim=1, act='sigmoid')\r\n",
    "    # 网络的前向计算函数\r\n",
    "    def forward(self, inputs):\r\n",
    "        fc1 = self.fc1(inputs)\r\n",
    "        fc2 = self.fc2(fc1)\r\n",
    "        x = self.fc3(fc2)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**调参：**   学习率和优化器也是个非常重要的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with fluid.dygraph.guard():\r\n",
    "    # 声明定义好的线性回归模型\r\n",
    "    model = Regressor(\"Regressor\")\r\n",
    "    # 开启模型训练模式\r\n",
    "    model.train()\r\n",
    "    # 定义优化算法，这里使用Adam Optimizer\r\n",
    "    # 学习率设置为0.00001\r\n",
    "    opt = fluid.optimizer.Adam(learning_rate=1e-5, parameter_list=model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 使用类别权重对数据不平衡问题进行处理\r\n",
    "def wce_loss(pred, label, w=48, epsilon=1e-05): # w 是给到 y=1 类别的权重，越大越重视\r\n",
    "    label = fluid.layers.clip(label, epsilon, 1-epsilon)\r\n",
    "    pred = fluid.layers.clip(pred, epsilon, 1-epsilon)\r\n",
    "\r\n",
    "    loss = -1 * (w * label * fluid.layers.log(pred) + (1 - label) * fluid.layers.log(1 - pred))\r\n",
    "    loss = fluid.layers.reduce_mean(loss)\r\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**调参：**  EPOCH_NUM、BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss is: [4.42944]\n",
      "epoch: 0, iter: 20, loss is: [4.258627]\n",
      "epoch: 0, iter: 40, loss is: [4.103505]\n",
      "epoch: 0, iter: 60, loss is: [4.135817]\n",
      "epoch: 0, iter: 80, loss is: [4.0722103]\n",
      "epoch: 0, iter: 100, loss is: [3.822112]\n",
      "epoch: 0, iter: 120, loss is: [4.0930786]\n",
      "epoch: 0, iter: 140, loss is: [3.872131]\n",
      "epoch: 0, iter: 160, loss is: [3.8494742]\n",
      "epoch: 0, iter: 180, loss is: [4.004653]\n",
      "epoch: 0, iter: 200, loss is: [3.912506]\n",
      "epoch: 0, iter: 220, loss is: [3.6657]\n",
      "epoch: 0, iter: 240, loss is: [3.5358813]\n",
      "epoch: 0, iter: 260, loss is: [3.611012]\n",
      "epoch: 0, iter: 280, loss is: [3.4558754]\n",
      "epoch: 0, iter: 300, loss is: [3.4343693]\n",
      "epoch: 0, iter: 320, loss is: [3.5103385]\n",
      "epoch: 0, iter: 340, loss is: [3.619368]\n",
      "epoch: 0, iter: 360, loss is: [3.47288]\n",
      "epoch: 0, iter: 380, loss is: [3.4157434]\n",
      "epoch: 0, iter: 400, loss is: [3.5165477]\n",
      "epoch: 0, iter: 420, loss is: [3.480499]\n",
      "epoch: 0, iter: 440, loss is: [3.307901]\n",
      "epoch: 0, iter: 460, loss is: [3.2342353]\n",
      "epoch: 0, iter: 480, loss is: [3.185408]\n",
      "epoch: 0, iter: 500, loss is: [3.2794774]\n",
      "epoch: 0, iter: 520, loss is: [3.3158126]\n",
      "epoch: 0, iter: 540, loss is: [3.3029685]\n",
      "epoch: 0, iter: 560, loss is: [3.2148452]\n",
      "epoch: 0, iter: 580, loss is: [3.1627612]\n",
      "epoch: 0, iter: 600, loss is: [3.2009904]\n",
      "epoch: 1, iter: 0, loss is: [3.1988013]\n",
      "epoch: 1, iter: 20, loss is: [3.1803572]\n",
      "epoch: 1, iter: 40, loss is: [3.168662]\n",
      "epoch: 1, iter: 60, loss is: [3.0997868]\n",
      "epoch: 1, iter: 80, loss is: [3.0573275]\n",
      "epoch: 1, iter: 100, loss is: [2.969334]\n",
      "epoch: 1, iter: 120, loss is: [2.9197633]\n",
      "epoch: 1, iter: 140, loss is: [2.9337626]\n",
      "epoch: 1, iter: 160, loss is: [3.0206888]\n",
      "epoch: 1, iter: 180, loss is: [2.9757552]\n",
      "epoch: 1, iter: 200, loss is: [2.8787274]\n",
      "epoch: 1, iter: 220, loss is: [2.901287]\n",
      "epoch: 1, iter: 240, loss is: [2.876462]\n",
      "epoch: 1, iter: 260, loss is: [2.8632555]\n",
      "epoch: 1, iter: 280, loss is: [2.8501027]\n",
      "epoch: 1, iter: 300, loss is: [2.788415]\n",
      "epoch: 1, iter: 320, loss is: [2.8617058]\n",
      "epoch: 1, iter: 340, loss is: [2.839023]\n",
      "epoch: 1, iter: 360, loss is: [2.7336147]\n",
      "epoch: 1, iter: 380, loss is: [2.84297]\n",
      "epoch: 1, iter: 400, loss is: [2.7991743]\n",
      "epoch: 1, iter: 420, loss is: [2.7781472]\n",
      "epoch: 1, iter: 440, loss is: [2.8493316]\n",
      "epoch: 1, iter: 460, loss is: [2.7597437]\n",
      "epoch: 1, iter: 480, loss is: [2.7043817]\n",
      "epoch: 1, iter: 500, loss is: [2.7531235]\n",
      "epoch: 1, iter: 520, loss is: [2.6957016]\n",
      "epoch: 1, iter: 540, loss is: [2.7835221]\n",
      "epoch: 1, iter: 560, loss is: [2.6866074]\n",
      "epoch: 1, iter: 580, loss is: [2.7039342]\n",
      "epoch: 1, iter: 600, loss is: [2.7110674]\n",
      "epoch: 2, iter: 0, loss is: [2.6847212]\n",
      "epoch: 2, iter: 20, loss is: [2.667614]\n",
      "epoch: 2, iter: 40, loss is: [2.6907384]\n",
      "epoch: 2, iter: 60, loss is: [2.7193172]\n",
      "epoch: 2, iter: 80, loss is: [2.7122173]\n",
      "epoch: 2, iter: 100, loss is: [2.7101903]\n",
      "epoch: 2, iter: 120, loss is: [2.660059]\n",
      "epoch: 2, iter: 140, loss is: [2.6848173]\n",
      "epoch: 2, iter: 160, loss is: [2.638083]\n",
      "epoch: 2, iter: 180, loss is: [2.6179514]\n",
      "epoch: 2, iter: 200, loss is: [2.6655161]\n",
      "epoch: 2, iter: 220, loss is: [2.638331]\n",
      "epoch: 2, iter: 240, loss is: [2.6333656]\n",
      "epoch: 2, iter: 260, loss is: [2.637709]\n",
      "epoch: 2, iter: 280, loss is: [2.609819]\n",
      "epoch: 2, iter: 300, loss is: [2.5693684]\n",
      "epoch: 2, iter: 320, loss is: [2.6342418]\n",
      "epoch: 2, iter: 340, loss is: [2.6273618]\n",
      "epoch: 2, iter: 360, loss is: [2.6742954]\n",
      "epoch: 2, iter: 380, loss is: [2.60153]\n",
      "epoch: 2, iter: 400, loss is: [2.587655]\n",
      "epoch: 2, iter: 420, loss is: [2.5809069]\n",
      "epoch: 2, iter: 440, loss is: [2.5957756]\n",
      "epoch: 2, iter: 460, loss is: [2.5915494]\n",
      "epoch: 2, iter: 480, loss is: [2.6108775]\n",
      "epoch: 2, iter: 500, loss is: [2.5784378]\n",
      "epoch: 2, iter: 520, loss is: [2.5320463]\n",
      "epoch: 2, iter: 540, loss is: [2.6280427]\n",
      "epoch: 2, iter: 560, loss is: [2.6392424]\n",
      "epoch: 2, iter: 580, loss is: [2.6157994]\n",
      "epoch: 2, iter: 600, loss is: [2.5911205]\n",
      "epoch: 3, iter: 0, loss is: [2.5915737]\n",
      "epoch: 3, iter: 20, loss is: [2.6040287]\n",
      "epoch: 3, iter: 40, loss is: [2.5701077]\n",
      "epoch: 3, iter: 60, loss is: [2.5837958]\n",
      "epoch: 3, iter: 80, loss is: [2.5808313]\n",
      "epoch: 3, iter: 100, loss is: [2.5558188]\n",
      "epoch: 3, iter: 120, loss is: [2.59581]\n",
      "epoch: 3, iter: 140, loss is: [2.579187]\n",
      "epoch: 3, iter: 160, loss is: [2.5672653]\n",
      "epoch: 3, iter: 180, loss is: [2.5505404]\n",
      "epoch: 3, iter: 200, loss is: [2.5484338]\n",
      "epoch: 3, iter: 220, loss is: [2.5426435]\n",
      "epoch: 3, iter: 240, loss is: [2.5692604]\n",
      "epoch: 3, iter: 260, loss is: [2.5954614]\n",
      "epoch: 3, iter: 280, loss is: [2.5397787]\n",
      "epoch: 3, iter: 300, loss is: [2.5520976]\n",
      "epoch: 3, iter: 320, loss is: [2.5909667]\n",
      "epoch: 3, iter: 340, loss is: [2.574109]\n",
      "epoch: 3, iter: 360, loss is: [2.5600736]\n",
      "epoch: 3, iter: 380, loss is: [2.5769134]\n",
      "epoch: 3, iter: 400, loss is: [2.5223298]\n",
      "epoch: 3, iter: 420, loss is: [2.563875]\n",
      "epoch: 3, iter: 440, loss is: [2.521081]\n",
      "epoch: 3, iter: 460, loss is: [2.5803275]\n",
      "epoch: 3, iter: 480, loss is: [2.5236812]\n",
      "epoch: 3, iter: 500, loss is: [2.4863682]\n",
      "epoch: 3, iter: 520, loss is: [2.5475745]\n",
      "epoch: 3, iter: 540, loss is: [2.5504644]\n",
      "epoch: 3, iter: 560, loss is: [2.5574033]\n",
      "epoch: 3, iter: 580, loss is: [2.5576835]\n",
      "epoch: 3, iter: 600, loss is: [2.5329227]\n",
      "epoch: 4, iter: 0, loss is: [2.5435262]\n",
      "epoch: 4, iter: 20, loss is: [2.5303934]\n",
      "epoch: 4, iter: 40, loss is: [2.5659971]\n",
      "epoch: 4, iter: 60, loss is: [2.5471263]\n",
      "epoch: 4, iter: 80, loss is: [2.5704193]\n",
      "epoch: 4, iter: 100, loss is: [2.5799677]\n",
      "epoch: 4, iter: 120, loss is: [2.565607]\n",
      "epoch: 4, iter: 140, loss is: [2.5512824]\n",
      "epoch: 4, iter: 160, loss is: [2.5601897]\n",
      "epoch: 4, iter: 180, loss is: [2.5719492]\n",
      "epoch: 4, iter: 200, loss is: [2.5530424]\n",
      "epoch: 4, iter: 220, loss is: [2.523246]\n",
      "epoch: 4, iter: 240, loss is: [2.5570512]\n",
      "epoch: 4, iter: 260, loss is: [2.5506196]\n",
      "epoch: 4, iter: 280, loss is: [2.5565994]\n",
      "epoch: 4, iter: 300, loss is: [2.509623]\n",
      "epoch: 4, iter: 320, loss is: [2.5400348]\n",
      "epoch: 4, iter: 340, loss is: [2.564713]\n",
      "epoch: 4, iter: 360, loss is: [2.5713582]\n",
      "epoch: 4, iter: 380, loss is: [2.5285807]\n",
      "epoch: 4, iter: 400, loss is: [2.5369325]\n",
      "epoch: 4, iter: 420, loss is: [2.508027]\n",
      "epoch: 4, iter: 440, loss is: [2.5003011]\n",
      "epoch: 4, iter: 460, loss is: [2.5830162]\n",
      "epoch: 4, iter: 480, loss is: [2.5570328]\n",
      "epoch: 4, iter: 500, loss is: [2.515736]\n",
      "epoch: 4, iter: 520, loss is: [2.567828]\n",
      "epoch: 4, iter: 540, loss is: [2.5540009]\n",
      "epoch: 4, iter: 560, loss is: [2.572392]\n",
      "epoch: 4, iter: 580, loss is: [2.5089734]\n",
      "epoch: 4, iter: 600, loss is: [2.4653945]\n",
      "epoch: 5, iter: 0, loss is: [2.5516064]\n",
      "epoch: 5, iter: 20, loss is: [2.4856687]\n",
      "epoch: 5, iter: 40, loss is: [2.4705305]\n",
      "epoch: 5, iter: 60, loss is: [2.4916735]\n",
      "epoch: 5, iter: 80, loss is: [2.524979]\n",
      "epoch: 5, iter: 100, loss is: [2.502071]\n",
      "epoch: 5, iter: 120, loss is: [2.5183682]\n",
      "epoch: 5, iter: 140, loss is: [2.523415]\n",
      "epoch: 5, iter: 160, loss is: [2.5268009]\n",
      "epoch: 5, iter: 180, loss is: [2.5165966]\n",
      "epoch: 5, iter: 200, loss is: [2.5210767]\n",
      "epoch: 5, iter: 220, loss is: [2.5120895]\n",
      "epoch: 5, iter: 240, loss is: [2.519445]\n",
      "epoch: 5, iter: 260, loss is: [2.539416]\n",
      "epoch: 5, iter: 280, loss is: [2.4727497]\n",
      "epoch: 5, iter: 300, loss is: [2.5423467]\n",
      "epoch: 5, iter: 320, loss is: [2.4651122]\n",
      "epoch: 5, iter: 340, loss is: [2.5077655]\n",
      "epoch: 5, iter: 360, loss is: [2.4949577]\n",
      "epoch: 5, iter: 380, loss is: [2.45011]\n",
      "epoch: 5, iter: 400, loss is: [2.5068858]\n",
      "epoch: 5, iter: 420, loss is: [2.501203]\n",
      "epoch: 5, iter: 440, loss is: [2.4650543]\n",
      "epoch: 5, iter: 460, loss is: [2.478316]\n",
      "epoch: 5, iter: 480, loss is: [2.5221665]\n",
      "epoch: 5, iter: 500, loss is: [2.4936686]\n",
      "epoch: 5, iter: 520, loss is: [2.5436144]\n",
      "epoch: 5, iter: 540, loss is: [2.4368472]\n",
      "epoch: 5, iter: 560, loss is: [2.5031846]\n",
      "epoch: 5, iter: 580, loss is: [2.4799316]\n",
      "epoch: 5, iter: 600, loss is: [2.529571]\n",
      "epoch: 6, iter: 0, loss is: [2.4825432]\n",
      "epoch: 6, iter: 20, loss is: [2.464524]\n",
      "epoch: 6, iter: 40, loss is: [2.4920838]\n",
      "epoch: 6, iter: 60, loss is: [2.508825]\n",
      "epoch: 6, iter: 80, loss is: [2.5163727]\n",
      "epoch: 6, iter: 100, loss is: [2.5375862]\n",
      "epoch: 6, iter: 120, loss is: [2.4680305]\n",
      "epoch: 6, iter: 140, loss is: [2.480636]\n",
      "epoch: 6, iter: 160, loss is: [2.4655433]\n",
      "epoch: 6, iter: 180, loss is: [2.5262227]\n",
      "epoch: 6, iter: 200, loss is: [2.4812026]\n",
      "epoch: 6, iter: 220, loss is: [2.5281088]\n",
      "epoch: 6, iter: 240, loss is: [2.5037098]\n",
      "epoch: 6, iter: 260, loss is: [2.477804]\n",
      "epoch: 6, iter: 280, loss is: [2.4613986]\n",
      "epoch: 6, iter: 300, loss is: [2.5038557]\n",
      "epoch: 6, iter: 320, loss is: [2.518665]\n",
      "epoch: 6, iter: 340, loss is: [2.464855]\n",
      "epoch: 6, iter: 360, loss is: [2.5096679]\n",
      "epoch: 6, iter: 380, loss is: [2.4489598]\n",
      "epoch: 6, iter: 400, loss is: [2.484274]\n",
      "epoch: 6, iter: 420, loss is: [2.4817872]\n",
      "epoch: 6, iter: 440, loss is: [2.510647]\n",
      "epoch: 6, iter: 460, loss is: [2.4844055]\n",
      "epoch: 6, iter: 480, loss is: [2.532532]\n",
      "epoch: 6, iter: 500, loss is: [2.539002]\n",
      "epoch: 6, iter: 520, loss is: [2.482578]\n",
      "epoch: 6, iter: 540, loss is: [2.5019777]\n",
      "epoch: 6, iter: 560, loss is: [2.4799938]\n",
      "epoch: 6, iter: 580, loss is: [2.4510765]\n",
      "epoch: 6, iter: 600, loss is: [2.481984]\n",
      "模型保存成功，模型参数保存在MLP_model中\n"
     ]
    }
   ],
   "source": [
    "# 模型训练和保存\r\n",
    "with dygraph.guard(fluid.CPUPlace()):\r\n",
    "    EPOCH_NUM = 7   # 设置外层循环次数,初始为6\r\n",
    "    BATCH_SIZE = 4096  # 设置batch大小，初始为4096。\r\n",
    "    \r\n",
    "    # 定义外层循环\r\n",
    "    for epoch_id in range(EPOCH_NUM):\r\n",
    "        # 在每轮迭代开始之前，将训练数据的顺序随机的打乱\r\n",
    "        np.random.shuffle(training_data)\r\n",
    "        # 将训练数据进行拆分\r\n",
    "        mini_batches = [training_data[k:k+BATCH_SIZE] for k in range(0, len(training_data), BATCH_SIZE)]\r\n",
    "        \r\n",
    "        # 定义内层循环\r\n",
    "        for iter_id, mini_batch in enumerate(mini_batches):\r\n",
    "            x = np.array(mini_batch[:, :-1]).astype('float32') # 获得当前批次训练数据\r\n",
    "            y = np.array(mini_batch[:, -1:]).astype('float32') # 获得当前批次训练标签\r\n",
    "\r\n",
    "            # 将numpy数据转为飞桨动态图variable形式\r\n",
    "            buyer_features = dygraph.to_variable(x)\r\n",
    "            result = dygraph.to_variable(y)\r\n",
    "            \r\n",
    "            # 前向计算\r\n",
    "            predicts = model(buyer_features)\r\n",
    "            # loss = fluid.layers.log_loss(predicts, prices)\r\n",
    "            loss = wce_loss(predicts, result)\r\n",
    "            avg_loss = fluid.layers.mean(loss)\r\n",
    "            \r\n",
    "            # logloss = fluid.layers.log_loss(predicts, prices)\r\n",
    "\r\n",
    "            if iter_id % 20 == 0:\r\n",
    "                print(\"epoch: {}, iter: {}, loss is: {}\".format(epoch_id, iter_id, avg_loss.numpy()))\r\n",
    "                # print(predicts)\r\n",
    "     \r\n",
    "            # 反向传播\r\n",
    "            avg_loss.backward()\r\n",
    "            # 最小化loss,更新参数\r\n",
    "            opt.minimize(avg_loss)\r\n",
    "            # 清除梯度\r\n",
    "            model.clear_gradients()\r\n",
    "    # 保存模型\r\n",
    "    fluid.save_dygraph(model.state_dict(), 'MLP_model')\r\n",
    "    print(\"模型保存成功，模型参数保存在MLP_model中\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 生成提交文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference result is [[0.64143705]\n",
      " [0.923506  ]\n",
      " [0.92082185]\n",
      " [0.6048201 ]\n",
      " [0.5992387 ]]\n"
     ]
    }
   ],
   "source": [
    "with dygraph.guard():\r\n",
    "    # 参数为保存模型参数的文件地址\r\n",
    "    model_dict, _ = fluid.load_dygraph('MLP_model')\r\n",
    "    model.load_dict(model_dict)\r\n",
    "    model.eval()\r\n",
    "    pre = pre_data.astype('float32')\r\n",
    "    # 将数据转为动态图的variable格式\r\n",
    "    pre = dygraph.to_variable(pre)\r\n",
    "    results = model(pre)\r\n",
    "\r\n",
    "    print(\"Inference result is {}\".format(results.numpy()[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685471\n"
     ]
    }
   ],
   "source": [
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64143705, 0.923506  , 0.92082185, 0.6048201 , 0.5992387 ,\n",
       "       0.9224564 ], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.numpy().flatten()[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('X_test.csv', usecols=['customer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_preds = pd.DataFrame(\r\n",
    "{    \"customer_id\": sub.customer_id, \r\n",
    "    \"pred\": results.numpy().flatten()}\r\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000014</td>\n",
       "      <td>0.641437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000034</td>\n",
       "      <td>0.923506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000046</td>\n",
       "      <td>0.920822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000069</td>\n",
       "      <td>0.604820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000105</td>\n",
       "      <td>0.599239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id      pred\n",
       "0      1000014  0.641437\n",
       "1      1000034  0.923506\n",
       "2      1000046  0.920822\n",
       "3      1000069  0.604820\n",
       "4      1000105  0.599239"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('data/data19383/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1585986 entries, 0 to 1585985\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count    Dtype  \n",
      "---  ------       --------------    -----  \n",
      " 0   customer_id  1585986 non-null  int64  \n",
      " 1   result       1585986 non-null  float64\n",
      "dtypes: float64(1), int64(1)\n",
      "memory usage: 24.2 MB\n"
     ]
    }
   ],
   "source": [
    "sub.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.merge(sub, df_preds, on='customer_id', how='left')\r\n",
    "submission.fillna(0,inplace=True)\r\n",
    "submission = submission[['customer_id','pred']]\r\n",
    "submission.rename(columns={'customer_id':'customer_id','pred':'result'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000014</td>\n",
       "      <td>0.641437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000034</td>\n",
       "      <td>0.923506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000046</td>\n",
       "      <td>0.920822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000048</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id    result\n",
       "0      1000000  0.000000\n",
       "1      1000014  0.641437\n",
       "2      1000034  0.923506\n",
       "3      1000046  0.920822\n",
       "4      1000048  0.000000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 将概率值转换为用户是否购买的标签\r\n",
    "# 由于输出的为0或1，设置阈值为0.5\r\n",
    "def f(x):\r\n",
    "    if x <= 0.5:\r\n",
    "        return 0\r\n",
    "    if x > 0.5:\r\n",
    "        return 1\r\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission['result'] = submission['result'].map(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 生成结果文件\r\n",
    "submission.to_csv('result.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**预测结束后提交result.csv文件即可！**\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/54d0499de8d54a8f90de35afbfd48e52ef35863775ab4e72974c8f21d65f2797)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "代码导出：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/c9f4d5ed4e9d444c8908c4d243444b275ac9854f15d744aba24dd6e62bf110bd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
